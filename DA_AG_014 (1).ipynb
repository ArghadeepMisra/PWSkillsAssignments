{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning | Assignment"
      ],
      "metadata": {
        "id": "R-nwEBskaa5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "CHbif0CUara3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment Code:\n",
        "## DA-AG-014"
      ],
      "metadata": {
        "id": "9g4FKEUKabKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arghadeep Misra\n",
        "### arghadeepmisra@gmail.com"
      ],
      "metadata": {
        "id": "EJ6d-eyxauPD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f6a90f5"
      },
      "source": [
        "---\n",
        "\n",
        "**Question 1:** What is Ensemble Learning in machine learning? Explain the key idea behind it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer"
      ],
      "metadata": {
        "id": "W0snAQHVVCQz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3747b3ad"
      },
      "source": [
        "Ensemble learning is a method in machine learning where we combine more than one model to make predictions. Instead of depending on a single model, we use a group of models together. This group is called an “ensemble.”\n",
        "\n",
        "The key idea is that many weak learners (models that are not very strong on their own) can work together to make a strong learner. Each model may make some mistakes, but when we combine their results, the overall prediction becomes more accurate and stable.\n",
        "\n",
        "Example: Think of a classroom. If one student answers a tough question, he might be wrong. But if the whole class votes, the majority answer is more likely to be correct. In the same way, ensemble learning reduces error and avoids overfitting compared to a single model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca7aede3"
      },
      "source": [
        "---\n",
        "\n",
        "**Question 2:** What is the difference between Bagging and Boosting?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer"
      ],
      "metadata": {
        "id": "EseYvyckVDtS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c759034"
      },
      "source": [
        "Bagging and Boosting are two types of ensemble methods, but they work in different ways.\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "It trains many models on random samples of the dataset (taken with replacement). Each model is trained independently. At the end, predictions are combined, usually by majority vote (for classification) or averaging (for regression).\n",
        "Example: Random Forest is a bagging method.\n",
        "\n",
        "Boosting:\n",
        "It trains models one after another. Each new model tries to fix the mistakes made by the previous one. More weight is given to the wrong predictions so the next model focuses on them. At the end, the models are combined in a weighted manner.\n",
        "Example: AdaBoost, XGBoost.\n",
        "\n",
        "Key difference: Bagging reduces variance by averaging many independent models. Boosting reduces bias by learning from mistakes step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "628acb47"
      },
      "source": [
        "---\n",
        "\n",
        "**Question 3:** What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer"
      ],
      "metadata": {
        "id": "qGrRh1gjVGFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bootstrap sampling means taking random samples from the dataset with replacement.\n",
        "“With replacement” means the same data point can appear more than once in the sample.\n",
        "\n",
        "In Bagging, like in Random Forest, each model (tree) is trained on a different bootstrap sample of the data. Since every model sees a slightly different dataset, they all learn differently. Later, their results are combined.\n",
        "\n",
        "The role of bootstrap sampling is to bring diversity among the models. If all models trained on the exact same data, they would give very similar outputs. By using bootstrap, we get models that make different errors, and when we combine them, the overall error reduces."
      ],
      "metadata": {
        "id": "XsYY_DP0V_0u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10f2b6a8"
      },
      "source": [
        "---\n",
        "\n",
        "**Question 4:** What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer"
      ],
      "metadata": {
        "id": "HL10imwAWVZs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecd3cc62"
      },
      "source": [
        "When we do bootstrap sampling, some data points don’t get picked in the sample. These leftover points are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "In Bagging methods like Random Forest, we can use these OOB samples to test the model that was trained on the bootstrap sample. This way, each model gets tested on data it never saw during training.\n",
        "\n",
        "The OOB score is the average accuracy (or error) measured on these OOB samples. It works like a built-in cross validation, so we don’t always need to keep a separate validation set.\n",
        "\n",
        "Example: If 30% of the data was not chosen in the bootstrap, those points become OOB for that tree. The model’s prediction on them helps calculate OOB score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1768c3ac"
      },
      "source": [
        "---\n",
        "\n",
        "**Question 5:** Compare feature importance analysis in a single Decision Tree vs. a Random Forest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "236dfce0"
      },
      "source": [
        "Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a single Decision Tree, feature importance is based on how much each feature reduces impurity (like Gini or entropy) when it splits the data. If a feature is used near the top of the tree and reduces impurity a lot, it gets high importance.\n",
        "\n",
        "But a single tree can be unstable. A small change in data may change the structure of the tree, and so the feature importance can also change a lot.\n",
        "\n",
        "In a Random Forest, feature importance is calculated by averaging across many trees. Each tree gives its own importance scores, and then we combine them. This makes the importance values more stable and reliable.\n",
        "\n",
        "So,\n",
        "\n",
        "Single tree - importance may be biased and unstable.\n",
        "\n",
        "Random forest - importance is averaged across many trees, so it is more consistent and trustworthy."
      ],
      "metadata": {
        "id": "vQcs59zzZOJv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffd13c8d"
      },
      "source": [
        "---\n",
        "\n",
        "**Question 6:** Write a Python program to:\n",
        "● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "(Include your Python code and output in the code box below.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "feat_imp = pd.DataFrame({'Feature': X.columns, 'Importance': model.feature_importances_})\n",
        "feat_imp = feat_imp.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(feat_imp.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eHYA5WsZjjk",
        "outputId": "72d66fc9-1a54-4f25-ed3b-32815f4415e5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Feature  Importance\n",
            "23            worst area    0.153892\n",
            "27  worst concave points    0.144663\n",
            "7    mean concave points    0.106210\n",
            "20          worst radius    0.077987\n",
            "6         mean concavity    0.068001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b5941e3"
      },
      "source": [
        "---\n",
        "\n",
        "**Question 7:** Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n",
        "(Include your Python code and output in the code box below.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "y_pred_tree = tree.predict(X_test)\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_tree))\n",
        "\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "print(\"Bagging Accuracy:\", accuracy_score(y_test, y_pred_bag))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhyCS2wFZ2y9",
        "outputId": "be3c1230-6207-4ed2-ec7e-fb0e74b665b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c59a24b"
      },
      "source": [
        "---\n",
        "\n",
        "**Question 8:** Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy\n",
        "(Include your Python code and output in the code box below.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, None],\n",
        "    'n_estimators': [50, 100, 150]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G_enMLyZ9zP",
        "outputId": "ee1edb62-3c64-46e4-a2d1-df679febd5a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'max_depth': 7, 'n_estimators': 50}\n",
            "Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e36a2ce"
      },
      "source": [
        "---\n",
        "\n",
        "**Question 9:** Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "(Include your Python code and output in the code box below.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "bag_model = BaggingRegressor(random_state=42)\n",
        "bag_model.fit(X_train, y_train)\n",
        "y_pred_bag = bag_model.predict(X_test)\n",
        "print(\"Bagging MSE:\", mean_squared_error(y_test, y_pred_bag))\n",
        "\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "print(\"Random Forest MSE:\", mean_squared_error(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_398lcXbaGRT",
        "outputId": "f76ace81-a0d6-4c3d-a653-f9bbd14ab64e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging MSE: 0.2824242776841025\n",
            "Random Forest MSE: 0.2553684927247781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80f30ffd"
      },
      "source": [
        "---\n",
        "\n",
        "**Question 10:** You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance.\n",
        "\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "110ecfb3"
      },
      "source": [
        "Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Choose between Bagging or Boosting\n",
        "\n",
        "Loan default is a complex problem with many patterns.\n",
        "\n",
        "Boosting is better because it focuses on hard-to-predict cases and usually gives higher accuracy.\n",
        "\n",
        "Step 2: Handle Overfitting\n",
        "\n",
        "Use parameters like max_depth to keep trees small.\n",
        "\n",
        "Use cross validation and early stopping (for boosting methods) to avoid overfitting.\n",
        "\n",
        "Step 3: Select Base Models\n",
        "\n",
        "Decision Trees are the base models because they are simple and fast.\n",
        "\n",
        "Step 4: Evaluate with Cross Validation\n",
        "\n",
        "Use cross validation to check if the model performs well across different data splits.\n",
        "\n",
        "Step 5: Justify\n",
        "\n",
        "Ensemble methods improve decision-making because they combine many weak learners and reduce both error and variance. This gives more reliable loan default predictions."
      ],
      "metadata": {
        "id": "r8-GFAf7aRRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# sample dataset (breast cancer) is used as placeholder\n",
        "# because financial loan dataset is not available in sklearn\n",
        "# process will be the same for loan default data\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = GradientBoostingClassifier(random_state=42)\n",
        "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "\n",
        "print(\"Cross-Validation Accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOmI1jFZaUL0",
        "outputId": "b0cb7de1-cb7b-43e3-b7b1-53ca1475b833"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Accuracy: 0.9516483516483516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n"
      ],
      "metadata": {
        "id": "PsCyc4BOaYXR"
      }
    }
  ]
}