{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Tree | Assignment**"
      ],
      "metadata": {
        "id": "NwwJqbg4LHvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Assignment Code: DA-AG-012*"
      ],
      "metadata": {
        "id": "aFJGFmzTLOSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is a Decision Tree, and how does it work in the context of classification?**"
      ],
      "metadata": {
        "id": "xN7dkTKsLabO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision tree is basically like a flow chart where we split data step by step using some rules. At the top we have root node, then it breaks into branches based on feature values, and finally we reach leaf node which gives us class label. For example if we want to classify whether a fruit is Apple or Orange, first split can be on color (Red or Orange). If color = Red, go left, if Orange go right. Then maybe check size or sweetness for next split. At the end, leaf node says \"Apple\" or \"Orange\".\n",
        "\n",
        "In classification, decision tree works by finding the best question (split) at each step. It chooses split by impurity measure like Gini or Entropy, whichever reduce impurity most. So data becomes more pure (similar class inside each branch). This keeps going until stopping condition like max depth or no further split possible.\n",
        "\n",
        "Main advantage is it is simple to understand, it feels like if-else condition. For example doctor can use decision tree to decide if patient has flu or not: check fever yes/no, then cough yes/no, then result. But disadvantage is tree can grow very complex and overfit data.\n",
        "\n",
        "So in short decision tree for classification is a supervised ML algorithm that learn rules from data and arrange them in tree structure to predict labels."
      ],
      "metadata": {
        "id": "tCg0HgUYNZur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**"
      ],
      "metadata": {
        "id": "JeL0wgqJNd5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(I couldn't find the Summation Symbol so uses E as Summation)\n",
        "\n",
        "When building a decision tree, we need to decide how to split the data at each step. For that we use impurity measures. Two most common are Gini Impurity and Entropy. Both basically tell us how \"mixed\" the classes are in a node.\n",
        "\n",
        "Gini Impurity: It shows how often a randomly chosen element would be misclassified if we assign label randomly according to distribution of labels in the node. Formula is 1 − E(pi^2), where pi is probability of class i. If node is pure (all samples same class), Gini = 0. If node is very mixed, value is higher.\n",
        "\n",
        "Entropy: This comes from information theory. Formula is −E(pi * log2(pi)). If all samples in node are same class, entropy = 0 (no uncertainty). If classes are equally distributed, entropy is maximum.\n",
        "\n",
        "How they impact split:\n",
        "When tree is splitting, it tries to reduce impurity. It checks each feature and possible cut point, then calculates impurity before and after split. The split that reduce impurity most is chosen. For example if using Gini, the algorithm pick the split that gives lowest Gini value after division. If using Entropy, it picks split with highest information gain (reduction of entropy).\n",
        "\n",
        "Example: Suppose we classify fruits into Apple and Orange. If one node has 10 apple and 0 orange, impurity is 0, so perfect. But if node has 5 apple and 5 orange, impurity is high, so tree will try to split further maybe on color to separate them better.\n",
        "\n",
        "So both Gini and Entropy serve same purpose: making nodes more pure. Gini is faster to compute, entropy more theoretical. But end result often similar in practice."
      ],
      "metadata": {
        "id": "2eduCepKNxfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**"
      ],
      "metadata": {
        "id": "kr7tledCOz1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees can easily become too big and complicated, which cause overfitting. To avoid that we use pruning. There are two types: pre-pruning and post-pruning.\n",
        "\n",
        "Pre-pruning means we stop tree from growing too much during the building process itself. For example, we can set max depth = 5, or say each node must have at least 10 samples to split. If these conditions not met, tree will stop growing further. Advantage of pre-pruning is it saves time and makes tree simpler from beginning itself. Also, it reduce overfitting early. Example: if we are making tree to predict if student pass/fail, we can stop splitting when only few students remain in one branch, because that split won’t generalize well.\n",
        "\n",
        "Post-pruning means we first allow tree to grow fully (like full big tree with many branches). Then afterwards we cut some branches which are not useful or too specific. This is done by checking performance on validation set. Advantage of post-pruning is that tree can explore more possible splits first, and then we remove only the unnecessary ones. So it usually gives better accuracy than pre-pruning. Example: in medical dataset, tree may create small branches only for 1-2 patients, but after pruning, those branches are removed since they don’t improve accuracy.\n",
        "\n",
        "In short, pre-pruning stops growth before it becomes too complex, while post-pruning cuts down extra complexity after full tree is built."
      ],
      "metadata": {
        "id": "4oE9fuaDO2ns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?**"
      ],
      "metadata": {
        "id": "YnSY46_vPJNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information Gain is like a score that tells us how good a split is. It basically measure how much “uncertainty” or impurity got reduced when we split a node. If the split makes the groups more pure (closer to single class), then the information gain is high.\n",
        "\n",
        "The formula is basically:\n",
        "Information Gain = impurity before split − impurity after split\n",
        "(since I dont have summation symbol, I write E as summation, so impurity after split = E(weight of branch * impurity of branch))\n",
        "\n",
        "For example, suppose in one node we have 50% apple and 50% orange. That node is very mixed, impurity high. If we split on color and get one branch 100% apple and other branch 100% orange, then impurity after split is 0. So information gain is maximum. That’s why tree will choose that split.\n",
        "\n",
        "Importance: Decision tree has to decide which feature and which value to split on at every step. It doesn’t do randomly. It looks at all possible splits, calculates information gain for each, and picks the split with highest gain. This way the tree learns rules that separate data in the most efficient way.\n",
        "\n",
        "Without using information gain (or gini), tree would not know which split is better. It might create useless splits and not classify well. So information gain is key because it makes sure tree reduce impurity step by step and becomes accurate.\n",
        "\n",
        "Example: If we classify students pass/fail, splitting on \"study hours\" might give higher information gain than splitting on \"shoe size\", so tree will pick \"study hours\" as first rule."
      ],
      "metadata": {
        "id": "IbW42yDiPNet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?**"
      ],
      "metadata": {
        "id": "Y-gCDcy-PUz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees are used in many real-life areas because they are simple and easy to understand. Few common applications are\n",
        "\n",
        "Healthcare: Doctors can use decision trees to predict if a patient has a disease. For example check symptoms like fever, cough, blood pressure step by step to reach a conclusion.\n",
        "\n",
        "Finance: Banks use decision trees to decide if they should give loan or not. They look at features like salary, past credit history, and existing debts.\n",
        "\n",
        "Marketing: Companies use decision trees to find which customers are likely to buy product. For example split customers by age, income, and previous purchase history.\n",
        "\n",
        "Education: Universities can use decision trees to predict if student will pass or fail based on attendance, assignments, and marks in exams.\n",
        "\n",
        "Advantages\n",
        "\n",
        "Very easy to understand, even for non technical people. It looks like if-else rules, so managers and doctors can read it.\n",
        "\n",
        "No need of scaling or normalizing data. It works on raw features directly.\n",
        "\n",
        "Can handle both categorical data (like yes/no, male/female) and numerical data (like age, salary).\n",
        "\n",
        "Limitations\n",
        "\n",
        "Trees can easily overfit, especially when they grow deep and memorize training data instead of learning patterns.\n",
        "\n",
        "Small changes in data can change the structure of the tree a lot, which means low stability.\n",
        "\n",
        "For continuous features, tree might not be as smooth as regression models, it creates step like decision boundaries.\n",
        "\n",
        "So in short, decision trees are very useful because of their simplicity and explainability, but they must be controlled with pruning or ensemble methods (like random forest) to avoid overfitting."
      ],
      "metadata": {
        "id": "o24kgOaWQNc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Write a Python program to:\n",
        "Load the Iris Dataset\n",
        "Train a Decision Tree Classifier using the Gini criterion\n",
        "Print the model’s accuracy and feature importances**"
      ],
      "metadata": {
        "id": "dlFgW-NgQbBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uylnRAURQj6",
        "outputId": "e0651600-a5ac-4b8e-d6ad-d4f56c64c60c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01911002 0.89326355 0.08762643]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to: Load the Iris Dataset, Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.**"
      ],
      "metadata": {
        "id": "EynHdkeVRh1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "model_depth3.fit(X_train, y_train)\n",
        "y_pred_depth3 = model_depth3.predict(X_test)\n",
        "print(\"Accuracy with depth=3:\", accuracy_score(y_test, y_pred_depth3))\n",
        "\n",
        "model_full = DecisionTreeClassifier(random_state=42)\n",
        "model_full.fit(X_train, y_train)\n",
        "y_pred_full = model_full.predict(X_test)\n",
        "print(\"Accuracy with full tree:\", accuracy_score(y_test, y_pred_full))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJGuUhsxRmjq",
        "outputId": "f2ddb3a9-a83f-4ff9-9b1d-b9a0661e2082"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with depth=3: 1.0\n",
            "Accuracy with full tree: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to: Load the Boston Housing Dataset, Train a Decision Tree Regressor, Print the Mean Squared Error (MSE) and feature importances**"
      ],
      "metadata": {
        "id": "2MXjBdDwRrYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "california = fetch_california_housing()\n",
        "X_house = pd.DataFrame(california.data, columns=california.feature_names)\n",
        "y_house = pd.Series(california.target)\n",
        "\n",
        "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(X_house, y_house, test_size=0.3, random_state=42)\n",
        "\n",
        "reg_model = DecisionTreeRegressor(random_state=42)\n",
        "reg_model.fit(X_train_h, y_train_h)\n",
        "\n",
        "y_pred_h = reg_model.predict(X_test_h)\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test_h, y_pred_h))\n",
        "print(\"Feature Importances:\", reg_model.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCRJ_AHeR0XK",
        "outputId": "707330c8-9a51-4ceb-c5d2-6f19d44ce2fd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.5280096503174904\n",
            "Feature Importances: [0.52345628 0.05213495 0.04941775 0.02497426 0.03220553 0.13901245\n",
            " 0.08999238 0.08880639]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to: Load the Iris Dataset, Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV, Print the best parameters and the resulting model accuracy**"
      ],
      "metadata": {
        "id": "GjkXMy29SGBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    \"max_depth\": [2, 3, 4, 5, None],\n",
        "    \"min_samples_split\": [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg7lCmiPSOeS",
        "outputId": "b72db2a4-c794-49d0-cfcd-c6248b33da94"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Best Accuracy: 0.9428571428571428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to:**\n",
        "\n",
        "Handle the missing values\n",
        "\n",
        "Encode the categorical features\n",
        "\n",
        "Train a Decision Tree model\n",
        "\n",
        "Tune its hyperparameters\n",
        "\n",
        "Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world setting."
      ],
      "metadata": {
        "id": "12dn82ovSS5A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5RAzp4TASWFw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}